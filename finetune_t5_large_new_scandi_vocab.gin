from __gin__ import dynamic_registration

#include 't5x/examples/t5/mt5/base.gin'
include 't5x/examples/t5/t5_1_1/large.gin'
include 't5x/configs/runs/finetune.gin'

# Register necessary SeqIO Tasks/Mixtures.
import t5.data.mixtures

import seqio
import tasks


VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://nb-t5/t5/vocabs/wikipedia/no-da-en-sv-nn-is_32000_unigram.sp.model"
seqio.SentencePieceVocabulary.extra_ids = 100

# Pretrain from T5 vocab
#INITIAL_CHECKPOINT_PATH = "gs://t5-data/pretrained_models/t5x/t5_1_1_large/checkpoint_1000000"
#INITIAL_CHECKPOINT_PATH = "gs://nb-t5/t5/t5_1_1_large_from_t5_scandi_unigram/checkpoint_2000000"
INITIAL_CHECKPOINT_PATH = "gs://nb-t5/t5/t5_1_1_large_from_t5_scandi_unigram/checkpoint_5563000/"
MIXTURE_OR_TASK_NAME = "extended_span_corruption_pretrain_scandi_unigram"
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 512}
# TRAIN_STEPS = 2000000
#TRAIN_STEPS = 22000000  # Original 2000000 (1M over t5_1_1 1M large chekpoint). Let's try 20M (it should be 32 (2048/64) but it might overfit)
TRAIN_STEPS = 12000000  # Previous calculation was wrong. Batch size is 64 per core, so times 8 â†’ 2048/ (64 * 8) = 4M steps. Let's do 12 and finish there.

DROPOUT_RATE = 0.0
BATCH_SIZE = 64 # Default to 128. Oringal was trained on bs 2048 sq 512 for 1M steps up to 1T tokens
NUM_EMBEDDINGS = 32128  # 128 * math.ceil((32000 + 100) / 128) for TPU efficiency
